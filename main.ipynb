{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4325b300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook native\n",
    "#import os\n",
    "#os.environ[\"SWEEP_ID\"] = \"pwg5qe4n\"\n",
    "#os.environ[\"WANDB_DUMMY_RUN\"] = \"False\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e33069a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import os\n",
    "import sys\n",
    "from typing import Any, Dict, List, Tuple\n",
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.transforms as T\n",
    "import wandb\n",
    "from loguru import logger\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from torch.nn import BatchNorm1d, Linear, ModuleList, ReLU, Sequential\n",
    "from torch_geometric.data import Dataset\n",
    "from torch_geometric.datasets import MoleculeNet\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import (\n",
    "    GATv2Conv,\n",
    "    GCNConv,\n",
    "    GINEConv,\n",
    "    global_add_pool,\n",
    "    global_mean_pool,\n",
    ")\n",
    "import random\n",
    "\n",
    "logger.remove()\n",
    "logger.add(\n",
    "    sys.stderr,\n",
    "    format=\"<green>{time:mm:ss}</green> | <level>{message}</level>\",\n",
    "    level=\"INFO\",\n",
    ")\n",
    "\n",
    "SWEEP_CONFIG: Dict[str, Any] = {\n",
    "    \"method\": \"bayes\",\n",
    "    \"metric\": {\"name\": \"val_auc\", \"goal\": \"maximize\"},\n",
    "    \"parameters\": {\n",
    "        \"model_type\": {\"values\": [\"GCN_Classic\", \"GCN_Res\", \"GINE\", \"GATv2\"]},\n",
    "        \"num_layers\": {\"values\": [3, 4, 5]},\n",
    "        \"hidden_channels\": {\"values\": [64, 128, 256]},\n",
    "        \"heads\": {\"values\": [2, 4]},\n",
    "        \"dropout\": {\"min\": 0.1, \"max\": 0.6},\n",
    "        \"weight_decay\": {\"values\": [1e-4, 1e-5, 0.0]},\n",
    "        \"learning_rate\": {\"min\": 0.0001, \"max\": 0.005},\n",
    "        \"batch_size\": {\"values\": [32, 64, 128]},\n",
    "        \"consistency_weight\": {\"min\": 0.1, \"max\": 2.0},\n",
    "        \"teacher_alpha\": {\"min\": 0.95, \"max\": 0.999},\n",
    "        \"label_rate\": {\"values\": [0.1]},\n",
    "    },\n",
    "}\n",
    "\n",
    "random_seed = np.random.randint(0, 100000)\n",
    "\n",
    "class Config:\n",
    "    \"\"\"System and experiment configuration.\"\"\"\n",
    "\n",
    "    PROJECT: str = \"drug-discovery-ssl\"\n",
    "    ENTITY: str = \"REDACTED\"\n",
    "    DUMMY_RUN: bool = os.getenv(\"WANDB_DUMMY_RUN\", \"False\").lower() == \"true\"\n",
    "    DUMMY_SIZE: int = 2000\n",
    "    DATASET: str = \"BBBP\"\n",
    "    DEVICE: torch.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    EPOCHS: int = 100\n",
    "    SEED: int = random_seed\n",
    "\n",
    "\n",
    "def get_stratified_validation_split(\n",
    "    train_val_set: Dataset, val_ratio: float = 0.1\n",
    ") -> Tuple[Dataset, Dataset]:\n",
    "    \"\"\"Splits combined pool using stratification to ensure class balance.\"\"\"\n",
    "    # extract labels via iteration to handle subsets correctly\n",
    "    labels = [d.y.item() for d in train_val_set]\n",
    "    y_full = np.nan_to_num(np.array(labels), 0)\n",
    "\n",
    "    splitter = StratifiedShuffleSplit(\n",
    "        n_splits=1, test_size=val_ratio, random_state=Config.SEED\n",
    "    )\n",
    "    indices = np.arange(len(train_val_set))\n",
    "\n",
    "    for train_idx, val_idx in splitter.split(indices.reshape(-1, 1), y_full):\n",
    "        return train_val_set[train_idx], train_val_set[val_idx]\n",
    "\n",
    "    return train_val_set, train_val_set\n",
    "\n",
    "\n",
    "def get_scaffold_split(dataset: Dataset) -> Tuple[Dataset, Dataset, Dataset]:\n",
    "    \"\"\"Deterministic split via smiles sorting to enforce generalization (OOD).\"\"\"\n",
    "    if not hasattr(dataset.data, \"smiles\"):\n",
    "        logger.warning(\"data -> smiles missing: random split fallback\")\n",
    "        return dataset.shuffle(), dataset.shuffle(), dataset.shuffle()\n",
    "\n",
    "    smiles: List[str] = dataset.data.smiles[: len(dataset)]\n",
    "    perm: List[int] = sorted(range(len(dataset)), key=lambda k: smiles[k])\n",
    "\n",
    "    n: int = len(dataset)\n",
    "    train_end: int = int(0.8 * n)\n",
    "    val_end: int = int(0.9 * n)\n",
    "\n",
    "    # split: 80% pool | 10% unused | 10% strict OOD test\n",
    "    return (\n",
    "        dataset[perm[:train_end]],\n",
    "        dataset[perm[train_end:val_end]],\n",
    "        dataset[perm[val_end:]],\n",
    "    )\n",
    "\n",
    "\n",
    "def get_loaders(config: wandb.Config) -> Dict[str, Any]:\n",
    "    \"\"\"Prepares dataloaders with salt removal and scaffold splitting.\"\"\"\n",
    "    # transform: force_reload ensures lcc (salt removal) applies\n",
    "    dataset = MoleculeNet(\n",
    "        root=\"./data/MoleculeNet\",\n",
    "        name=Config.DATASET,\n",
    "        pre_transform=T.LargestConnectedComponents(),\n",
    "        force_reload=True,\n",
    "    )\n",
    "\n",
    "    if Config.DUMMY_RUN:\n",
    "        dataset = dataset[: Config.DUMMY_SIZE]\n",
    "        logger.warning(f\"mode -> dummy run active: subset to {len(dataset)} graphs\")\n",
    "    else:\n",
    "        logger.info(f\"mode -> production: full dataset ({len(dataset)} graphs)\")\n",
    "\n",
    "    # calc: imbalance weight\n",
    "    y_vals = [d.y.item() for d in dataset]\n",
    "    y_arr = np.nan_to_num(np.array(y_vals), 0)\n",
    "    pos_count = y_arr.sum()\n",
    "    neg_count = len(y_arr) - pos_count\n",
    "    pos_weight: float = neg_count / (pos_count + 1e-5)\n",
    "\n",
    "    # splits\n",
    "    train_val_pool, _, test_ds = get_scaffold_split(dataset)\n",
    "    train_ds, val_ds = get_stratified_validation_split(train_val_pool, val_ratio=0.11)\n",
    "\n",
    "    # ssl scarcity\n",
    "    train_ds = train_ds.shuffle()\n",
    "    n_lbl: int = int(config.label_rate * len(train_ds))\n",
    "\n",
    "    return {\n",
    "        \"labeled\": DataLoader(\n",
    "            train_ds[:n_lbl],\n",
    "            batch_size=config.batch_size,\n",
    "            shuffle=True,\n",
    "            drop_last=True,\n",
    "        ),\n",
    "        \"unlabeled\": DataLoader(\n",
    "            train_ds[n_lbl:],\n",
    "            batch_size=config.batch_size,\n",
    "            shuffle=True,\n",
    "            drop_last=True,\n",
    "        ),\n",
    "        \"val\": DataLoader(val_ds, batch_size=config.batch_size),\n",
    "        \"test\": DataLoader(test_ds, batch_size=config.batch_size),\n",
    "        \"pos_weight\": torch.tensor([pos_weight]).to(Config.DEVICE),\n",
    "    }\n",
    "\n",
    "\n",
    "class BaseGNN(torch.nn.Module):\n",
    "    \"\"\"Abstract base for consistency in forward pass signature.\"\"\"\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        edge_index: torch.Tensor,\n",
    "        edge_attr: torch.Tensor,\n",
    "        batch: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class GCN_Classic(BaseGNN):\n",
    "    \"\"\"The Welling Replica (ICLR 2017). No residuals, mean pool.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_dim: int,\n",
    "        hidden: int,\n",
    "        out_dim: int,\n",
    "        edge_dim: int,\n",
    "        dropout: float,\n",
    "        layers: int,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.node_lin = Linear(in_dim, hidden)\n",
    "        self.convs = ModuleList()\n",
    "\n",
    "        for _ in range(layers):\n",
    "            self.convs.append(GCNConv(hidden, hidden))\n",
    "\n",
    "        self.lin = Linear(hidden, out_dim)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        edge_index: torch.Tensor,\n",
    "        edge_attr: torch.Tensor,\n",
    "        batch: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        x = self.node_lin(x.float())\n",
    "\n",
    "        for conv in self.convs:\n",
    "            x = conv(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "\n",
    "        x = global_mean_pool(x, batch)\n",
    "        return self.lin(x)\n",
    "\n",
    "\n",
    "class GCN_Res(BaseGNN):\n",
    "    \"\"\"Modern GCN baseline. Residuals, batchnorm, add pool.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_dim: int,\n",
    "        hidden: int,\n",
    "        out_dim: int,\n",
    "        edge_dim: int,\n",
    "        dropout: float,\n",
    "        layers: int,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.node_lin = Linear(in_dim, hidden)\n",
    "        self.convs = ModuleList()\n",
    "        self.bns = ModuleList()\n",
    "\n",
    "        for _ in range(layers):\n",
    "            self.convs.append(GCNConv(hidden, hidden))\n",
    "            self.bns.append(BatchNorm1d(hidden))\n",
    "\n",
    "        self.lin = Linear(hidden, out_dim)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        edge_index: torch.Tensor,\n",
    "        edge_attr: torch.Tensor,\n",
    "        batch: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        x = self.node_lin(x.float())\n",
    "\n",
    "        for conv, bn in zip(self.convs, self.bns):\n",
    "            identity = x\n",
    "            x = conv(x, edge_index)\n",
    "            x = bn(x)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "            x = x + identity\n",
    "\n",
    "        x = global_add_pool(x, batch)\n",
    "        return self.lin(x)\n",
    "\n",
    "\n",
    "class GINE(BaseGNN):\n",
    "    \"\"\"Graph Isomorphism Network + Edges. Maximally expressive.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_dim: int,\n",
    "        hidden: int,\n",
    "        out_dim: int,\n",
    "        edge_dim: int,\n",
    "        dropout: float,\n",
    "        layers: int,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.node_lin = Linear(in_dim, hidden)\n",
    "        self.edge_lin = Linear(edge_dim, hidden)\n",
    "        self.convs = ModuleList()\n",
    "        self.bns = ModuleList()\n",
    "\n",
    "        for _ in range(layers):\n",
    "            mlp = Sequential(\n",
    "                Linear(hidden, hidden),\n",
    "                BatchNorm1d(hidden),\n",
    "                ReLU(),\n",
    "                Linear(hidden, hidden),\n",
    "            )\n",
    "            self.convs.append(GINEConv(mlp, edge_dim=hidden))\n",
    "            self.bns.append(BatchNorm1d(hidden))\n",
    "\n",
    "        self.lin = Linear(hidden, out_dim)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        edge_index: torch.Tensor,\n",
    "        edge_attr: torch.Tensor,\n",
    "        batch: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        x = self.node_lin(x.float())\n",
    "        edge_attr = self.edge_lin(edge_attr.float())\n",
    "\n",
    "        for conv, bn in zip(self.convs, self.bns):\n",
    "            identity = x\n",
    "            x = conv(x, edge_index, edge_attr=edge_attr)\n",
    "            x = bn(x)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "            x = x + identity\n",
    "\n",
    "        x = global_add_pool(x, batch)\n",
    "        return self.lin(x)\n",
    "\n",
    "\n",
    "class GATv2(BaseGNN):\n",
    "    \"\"\"Dynamic Attention with Edge Features.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_dim: int,\n",
    "        hidden: int,\n",
    "        out_dim: int,\n",
    "        edge_dim: int,\n",
    "        dropout: float,\n",
    "        layers: int,\n",
    "        heads: int,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.node_lin = Linear(in_dim, hidden)\n",
    "        self.edge_lin = Linear(edge_dim, hidden)\n",
    "        self.convs = ModuleList()\n",
    "        self.bns = ModuleList()\n",
    "\n",
    "        for _ in range(layers):\n",
    "            self.convs.append(\n",
    "                GATv2Conv(hidden, hidden, heads=heads, concat=False, edge_dim=hidden)\n",
    "            )\n",
    "            self.bns.append(BatchNorm1d(hidden))\n",
    "\n",
    "        self.lin = Linear(hidden, out_dim)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        edge_index: torch.Tensor,\n",
    "        edge_attr: torch.Tensor,\n",
    "        batch: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        x = self.node_lin(x.float())\n",
    "        edge_attr = self.edge_lin(edge_attr.float())\n",
    "\n",
    "        for conv, bn in zip(self.convs, self.bns):\n",
    "            identity = x\n",
    "            x = conv(x, edge_index, edge_attr=edge_attr)\n",
    "            x = bn(x)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "            x = x + identity\n",
    "\n",
    "        x = global_add_pool(x, batch)\n",
    "        return self.lin(x)\n",
    "\n",
    "\n",
    "def get_model(config: wandb.Config, in_dim: int, edge_dim: int) -> BaseGNN:\n",
    "    \"\"\"Factory dispatch for architecture selection.\"\"\"\n",
    "    if config.model_type == \"GCN_Classic\":\n",
    "        return GCN_Classic(\n",
    "            in_dim,\n",
    "            config.hidden_channels,\n",
    "            1,\n",
    "            edge_dim,\n",
    "            config.dropout,\n",
    "            config.num_layers,\n",
    "        )\n",
    "    elif config.model_type == \"GCN_Res\":\n",
    "        return GCN_Res(\n",
    "            in_dim,\n",
    "            config.hidden_channels,\n",
    "            1,\n",
    "            edge_dim,\n",
    "            config.dropout,\n",
    "            config.num_layers,\n",
    "        )\n",
    "    elif config.model_type == \"GINE\":\n",
    "        return GINE(\n",
    "            in_dim,\n",
    "            config.hidden_channels,\n",
    "            1,\n",
    "            edge_dim,\n",
    "            config.dropout,\n",
    "            config.num_layers,\n",
    "        )\n",
    "    elif config.model_type == \"GATv2\":\n",
    "        return GATv2(\n",
    "            in_dim,\n",
    "            config.hidden_channels,\n",
    "            1,\n",
    "            edge_dim,\n",
    "            config.dropout,\n",
    "            config.num_layers,\n",
    "            config.heads,\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"unknown model_type: {config.model_type}\")\n",
    "\n",
    "\n",
    "def update_teacher(\n",
    "    student: torch.nn.Module, teacher: torch.nn.Module, alpha: float\n",
    ") -> None:\n",
    "    for s, t in zip(student.parameters(), teacher.parameters()):\n",
    "        t.data.mul_(alpha).add_(s.data, alpha=1.0 - alpha)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_epoch(\n",
    "    student: BaseGNN,\n",
    "    teacher: BaseGNN,\n",
    "    loaders: Dict,\n",
    "    optimizer: torch.optim.Adam,\n",
    "    config: wandb.Config,\n",
    ") -> Dict[str, float]:\n",
    "    student.train()\n",
    "    teacher.train()\n",
    "    metrics = {\"loss\": 0.0, \"sup\": 0.0, \"cons\": 0.0}\n",
    "    iter_lbl = iter(loaders[\"labeled\"])\n",
    "\n",
    "    for batch_unlbl in loaders[\"unlabeled\"]:\n",
    "        try:\n",
    "            batch_lbl = next(iter_lbl)\n",
    "        except StopIteration:\n",
    "            iter_lbl = iter(loaders[\"labeled\"])\n",
    "            batch_lbl = next(iter_lbl)\n",
    "\n",
    "        # move to device\n",
    "        batch_lbl = batch_lbl.to(Config.DEVICE)\n",
    "        batch_unlbl = batch_unlbl.to(Config.DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # supervised\n",
    "        out_lbl = student(\n",
    "            batch_lbl.x,\n",
    "            batch_lbl.edge_index,\n",
    "            batch_lbl.edge_attr,\n",
    "            batch_lbl.batch,\n",
    "        )\n",
    "        mask = ~torch.isnan(batch_lbl.y)\n",
    "        loss_sup = F.binary_cross_entropy_with_logits(\n",
    "            out_lbl[mask],\n",
    "            batch_lbl.y[mask].float(),\n",
    "            pos_weight=loaders[\"pos_weight\"],\n",
    "        )\n",
    "\n",
    "        # consistency\n",
    "        out_student = student(\n",
    "            batch_unlbl.x,\n",
    "            batch_unlbl.edge_index,\n",
    "            batch_unlbl.edge_attr,\n",
    "            batch_unlbl.batch,\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            out_teacher = teacher(\n",
    "                batch_unlbl.x,\n",
    "                batch_unlbl.edge_index,\n",
    "                batch_unlbl.edge_attr,\n",
    "                batch_unlbl.batch,\n",
    "            )\n",
    "\n",
    "        loss_cons = F.mse_loss(torch.sigmoid(out_student), torch.sigmoid(out_teacher))\n",
    "\n",
    "        # optimize\n",
    "        loss = loss_sup + (config.consistency_weight * loss_cons)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        update_teacher(student, teacher, config.teacher_alpha)\n",
    "\n",
    "        metrics[\"loss\"] += loss.item()\n",
    "        metrics[\"sup\"] += loss_sup.item()\n",
    "        metrics[\"cons\"] += loss_cons.item()\n",
    "\n",
    "    steps = len(loaders[\"unlabeled\"])\n",
    "    return {k: v / steps for k, v in metrics.items()}\n",
    "\n",
    "\n",
    "# 1. Helper for Enrichment Factor\n",
    "def calculate_enrichment_factor(preds: np.ndarray, targets: np.ndarray, percentile: float = 0.05) -> float:\n",
    "    \"\"\"Calculates how many actives are in the top X% of predictions.\"\"\"\n",
    "    n_total = len(preds)\n",
    "    n_top = int(percentile * n_total)\n",
    "    \n",
    "    if n_top == 0: return 0.0\n",
    "    \n",
    "    # Sort descending\n",
    "    sorted_indices = np.argsort(preds)[::-1]\n",
    "    top_indices = sorted_indices[:n_top]\n",
    "    \n",
    "    n_actives_top = targets[top_indices].sum()\n",
    "    subset_hit_rate = n_actives_top / n_top\n",
    "    base_hit_rate = targets.sum() / n_total\n",
    "    \n",
    "    return subset_hit_rate / (base_hit_rate + 1e-9)\n",
    "\n",
    "# 2. Updated Evaluation Function\n",
    "@torch.no_grad()\n",
    "def evaluate(model: BaseGNN, loader: DataLoader, log_table: bool = False, split_name: str = \"val\") -> float:\n",
    "    model.eval()\n",
    "    preds, targets = [], []\n",
    "    \n",
    "    for batch in loader:\n",
    "        batch = batch.to(Config.DEVICE)\n",
    "        out = model(batch.x, batch.edge_index, batch.edge_attr, batch.batch)\n",
    "        preds.append(torch.sigmoid(out).cpu())\n",
    "        targets.append(batch.y.cpu())\n",
    "\n",
    "    cat_preds = torch.cat(preds).numpy().flatten()\n",
    "    cat_targets = torch.cat(targets).numpy().flatten()\n",
    "    \n",
    "    # Mask NaNs\n",
    "    mask = ~np.isnan(cat_targets)\n",
    "    clean_preds = cat_preds[mask]\n",
    "    clean_targets = cat_targets[mask]\n",
    "\n",
    "    try:\n",
    "        auc = roc_auc_score(clean_targets, clean_preds)\n",
    "    except ValueError:\n",
    "        return 0.5\n",
    "\n",
    "    # log deep metrics\n",
    "    if log_table:\n",
    "        hard_preds = (clean_preds > 0.5).astype(int)\n",
    "        prec = precision_score(clean_targets, hard_preds, zero_division=0)\n",
    "        rec = recall_score(clean_targets, hard_preds, zero_division=0)\n",
    "        ef_10 = calculate_enrichment_factor(clean_preds, clean_targets, percentile=0.1)\n",
    "        \n",
    "        # Log Scalar Metrics\n",
    "        logger.info(f\"test metrics -> auc: {auc:.4f} | ef@10%: {ef_10:.2f} | prec: {prec:.2f} | rec: {rec:.2f}\")\n",
    "        wandb.log({\n",
    "            f\"{split_name}_auc\": auc,\n",
    "            f\"{split_name}_ef_10\": ef_10,\n",
    "            f\"{split_name}_precision\": prec,\n",
    "            f\"{split_name}_recall\": rec\n",
    "        })\n",
    "        \n",
    "        # Log Raw Data Table (The \"Safety Net\")\n",
    "        table = wandb.Table(data=[[p, t] for p, t in zip(clean_preds, clean_targets)], columns=[\"pred_prob\", \"target\"])\n",
    "        wandb.log({f\"{split_name}_predictions\": table})\n",
    "        \n",
    "        # Log ROC Curve directly\n",
    "        wandb.log({f\"{split_name}_roc\": wandb.plot.roc_curve(clean_targets, np.stack([1-clean_preds, clean_preds], axis=1))})\n",
    "\n",
    "    return float(auc)\n",
    "\n",
    "\n",
    "def main_sweep() -> None:\n",
    "    project_name = Config.PROJECT\n",
    "    if Config.DUMMY_RUN:\n",
    "        project_name = f\"{project_name}_min\"\n",
    "\n",
    "    with wandb.init(project=project_name, entity=Config.ENTITY):\n",
    "        config = wandb.config\n",
    "        loaders = get_loaders(config)\n",
    "\n",
    "        # dynamic dim extraction\n",
    "        sample = next(iter(loaders[\"labeled\"]))\n",
    "        in_dim = sample.num_node_features\n",
    "        edge_dim = sample.num_edge_features\n",
    "\n",
    "        # factory init\n",
    "        student = get_model(config, in_dim, edge_dim).to(Config.DEVICE)\n",
    "        teacher = get_model(config, in_dim, edge_dim).to(Config.DEVICE)\n",
    "\n",
    "        teacher.load_state_dict(student.state_dict())\n",
    "        for p in teacher.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        optimizer = torch.optim.Adam(\n",
    "            student.parameters(),\n",
    "            lr=config.learning_rate,\n",
    "            weight_decay=config.weight_decay,\n",
    "        )\n",
    "\n",
    "        logger.info(\n",
    "            f\"start -> model: {config.model_type} | layers: {config.num_layers}\"\n",
    "        )\n",
    "\n",
    "        best_model_state = None\n",
    "        best_val_auc = 0.0\n",
    "\n",
    "        for epoch in range(Config.EPOCHS):\n",
    "            m = train_epoch(student, teacher, loaders, optimizer, config)\n",
    "            val_auc = evaluate(student, loaders[\"val\"], log_table=False)\n",
    "            if val_auc > best_val_auc:\n",
    "                best_val_auc = val_auc\n",
    "                best_model_state = copy.deepcopy(student.state_dict())\n",
    "\n",
    "            wandb.log({**m, \"val_auc\": val_auc, \"epoch\": epoch})\n",
    "\n",
    "        # test eval\n",
    "        if best_model_state is not None:\n",
    "            student.load_state_dict(best_model_state)\n",
    "            # ENABLE LOGGING HERE\n",
    "            evaluate(student, loaders[\"test\"], log_table=True, split_name=\"test\")\n",
    "            logger.success(f\"run complete\")\n",
    "        else:\n",
    "            logger.error(\"failed -> no model saved\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\" and False:\n",
    "    try:\n",
    "        import sklearn  # noqa: F401\n",
    "    except ImportError:\n",
    "        logger.error(\"missing -> pip install scikit-learn\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # DISTRIBUTED LOGIC:\n",
    "    # 1. Check if a SWEEP_ID is provided => Worker mode\n",
    "    # 2. If not, initialize a new sweep => Controller mode\n",
    "    if \"SWEEP_ID\" in os.environ:\n",
    "        sweep_id = os.environ[\"SWEEP_ID\"]\n",
    "        logger.info(f\"worker start -> joining sweep: {sweep_id}\")\n",
    "    else:\n",
    "        sweep_id = wandb.sweep(\n",
    "            SWEEP_CONFIG, project=Config.PROJECT, entity=Config.ENTITY\n",
    "        )\n",
    "        logger.info(f\"controller start -> init sweep: {sweep_id}\")\n",
    "        logger.info(\n",
    "            \"to join workers, run: export SWEEP_ID={sweep_id} && python main.py\"\n",
    "        )\n",
    "\n",
    "    wandb.agent(sweep_id, main_sweep, count=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fedd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_FINAL_EVALUATION = True\n",
    "\n",
    "if RUN_FINAL_EVALUATION:\n",
    "    import copy\n",
    "    import sys\n",
    "    from typing import Any, Dict\n",
    "\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    import wandb\n",
    "\n",
    "    # Best WandB run hyperparameters\n",
    "    BEST_PARAMS = {\n",
    "        \"model_type\": \"GCN_Res\",\n",
    "        \"num_layers\": 4,\n",
    "        \"hidden_channels\": 128,\n",
    "        \"heads\": 4,\n",
    "        \"dropout\": 0.3280387,\n",
    "        \"batch_size\": 32,\n",
    "        \"label_rate\": 0.1,\n",
    "        \"weight_decay\": 1e-05,\n",
    "        \"learning_rate\": 0.000692,\n",
    "        \"teacher_alpha\": 0.9905,\n",
    "        \"epochs\": 100,\n",
    "    }\n",
    "\n",
    "    BEST_PARAMS = {\n",
    "        \"heads\": 2,\n",
    "        \"dropout\": 0.5174957708684919,\n",
    "        \"batch_size\": 64,\n",
    "        \"label_rate\": 0.1,\n",
    "        \"model_type\": \"GCN_Res\",\n",
    "        \"num_layers\": 4,\n",
    "        \"weight_decay\": 0,\n",
    "        \"learning_rate\": 0.0014010278646211564,\n",
    "        \"teacher_alpha\": 0.9854702728613248,\n",
    "        \"hidden_channels\": 64,\n",
    "        \"consistency_weight\": 0.5082421350750431,\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "    # Proper seeded experiments\n",
    "    SEEDS = [0, 1, 2, 3, 4]\n",
    "    EXPERIMENTS = {\n",
    "        \"Baseline (No Teacher)\": {\"consistency_weight\": 0.0},\n",
    "        \"Mean Teacher\": {\"consistency_weight\": 1.03712},\n",
    "    }\n",
    "\n",
    "    def train_one_run(run_config: Dict[str, Any]) -> float:\n",
    "        \"\"\"Executes full training lifecycle for a single seed configuration.\"\"\"\n",
    "        Config.SEED = run_config[\"seed\"]\n",
    "        torch.manual_seed(Config.SEED)\n",
    "        np.random.seed(Config.SEED)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed_all(Config.SEED)\n",
    "\n",
    "        mode = \"disabled\" if Config.DUMMY_RUN else \"online\"\n",
    "        run_name = f\"eval_lambda{run_config['consistency_weight']}_s{run_config['seed']}\"\n",
    "\n",
    "        with wandb.init(\n",
    "            project=Config.PROJECT,\n",
    "            entity=Config.ENTITY,\n",
    "            config=run_config,\n",
    "            reinit=True,\n",
    "            mode=mode,\n",
    "            name=run_name,\n",
    "        ) as run:\n",
    "            config = wandb.config\n",
    "            loaders = get_loaders(config)\n",
    "\n",
    "            # dimensions\n",
    "            sample = next(iter(loaders[\"labeled\"]))\n",
    "            in_dim = sample.num_node_features\n",
    "            edge_dim = sample.num_edge_features\n",
    "\n",
    "            # models & optimizer\n",
    "            student = get_model(config, in_dim, edge_dim).to(Config.DEVICE)\n",
    "            teacher = get_model(config, in_dim, edge_dim).to(Config.DEVICE)\n",
    "            teacher.load_state_dict(student.state_dict())\n",
    "\n",
    "            # freeze, teacher is ema only\n",
    "            for p in teacher.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "            optimizer = torch.optim.Adam(\n",
    "                student.parameters(),\n",
    "                lr=config.learning_rate,\n",
    "                weight_decay=config.weight_decay,\n",
    "            )\n",
    "\n",
    "            best_model_state = None\n",
    "            best_val_auc = 0.0\n",
    "\n",
    "            # note: uses config.epochs if present, else global default\n",
    "            epochs = getattr(config, \"epochs\", Config.EPOCHS)\n",
    "            for epoch in range(epochs):\n",
    "                metrics = train_epoch(student, teacher, loaders, optimizer, config)\n",
    "                val_auc = evaluate(student, loaders[\"val\"], log_table=False)\n",
    "\n",
    "                if val_auc > best_val_auc:\n",
    "                    best_val_auc = val_auc\n",
    "                    best_model_state = copy.deepcopy(student.state_dict())\n",
    "\n",
    "                # log: internal loop metrics\n",
    "                wandb.log({**metrics, \"val_auc\": val_auc, \"epoch\": epoch})\n",
    "\n",
    "            # eval: test set\n",
    "            if best_model_state is not None:\n",
    "                student.load_state_dict(best_model_state)\n",
    "                # log_table=True ensures rich media (roc, tables) are logged for final eval\n",
    "                test_auc = evaluate(\n",
    "                    student, loaders[\"test\"], log_table=True, split_name=\"test\"\n",
    "                )\n",
    "                return float(test_auc)\n",
    "\n",
    "            return 0.0\n",
    "\n",
    "    final_results = {}\n",
    "    for exp_name, specific_args in EXPERIMENTS.items():\n",
    "        print(f\"\\n>> Evaluating: {exp_name}\")\n",
    "        scores = []\n",
    "\n",
    "        for seed in SEEDS:\n",
    "            # Prepare the specific config for this run\n",
    "            run_config = copy.deepcopy(BEST_PARAMS)\n",
    "            run_config.update(specific_args)\n",
    "            run_config[\"seed\"] = seed\n",
    "\n",
    "            try:\n",
    "                test_auc = train_one_run(run_config)\n",
    "                scores.append(test_auc)\n",
    "                print(f\"Seed {seed} Test AUC: {test_auc:.4f}\")\n",
    "\n",
    "            except Exception as e:\n",
    "\n",
    "        # Aggregate stats\n",
    "        if scores:\n",
    "            mean_score = np.mean(scores)\n",
    "            std_score = np.std(scores)\n",
    "            final_results[exp_name] = (mean_score, std_score)\n",
    "            print(f\"Average: {mean_score:.4f} ± {std_score:.4f}\")\n",
    "\n",
    "\n",
    "    for name, (mean, std) in final_results.items():\n",
    "        print(f\"{name:<30} | {mean:.4f} ± {std:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DTU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
